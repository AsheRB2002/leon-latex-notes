\documentclass[landscape, 8pt]{extarticle}
\usepackage{geometry}
% \usepackage{showframe}
\usepackage[dvipsnames]{xcolor}

\colorlet{colour1}{Red}
\colorlet{colour2}{Green}
\colorlet{colour3}{Cerulean}

\geometry{
    a4paper, 
    margin=0.17in
}

\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
% \usepackage{preamble}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage[framemethod=TikZ]{mdframed}
% \usepackage{../thmboxes_white}
\usepackage{../thmboxes_v2}
\usepackage{float}
% \usepackage{setspace}
\usepackage[nodisplayskipstretch]{setspace}

% \setlength{\parskip}{0pt}

% Custom Definitions of operators
\DeclareMathOperator{\Ima}{im}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\send}{send}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Maps}{Maps}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\scale}{sc}

\usepackage{hyperref} % note: this is the final package

\parindent = 0pt

\renewcommand\labelitemi{\tiny$\bullet$}

\begin{document}

\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.5pt}
\setlength{\abovedisplayshortskip}{3.5pt}
\setlength{\belowdisplayshortskip}{3.5pt}

\begin{multicols}{3}
\raggedcolumns

\section{Vector Spaces}
\subsection{Fields and Vector Spaces}

\begin{rcl}[Definition of a Group]{rcl:group-def}{A}
    For an operation $\ast$, We say a non-empty set G is a \textbf{group} under $\ast$ if the following four axioms hold:
    \begin{itemize}
        \setlength\itemsep{0em}
        \item \textbf{G1 - Closure:} $a\ast b \in G$ for all $a,b\in G$.
        \item \textbf{G2 - Associativity:} $(a\ast b) \ast c =a\ast(b\ast c)$ for all $a,b,c\in G$
        \item \textbf{G3 - Identity:} There exists an \textit{identity} element of $G$ such that $e\ast g = g\ast e = g$ for all $g\in G$.
        \item \textbf{G4 - Inverse:} Every element $g\in G$ has an \textit{inverse} $g^{-1}$ such that $g\ast g^{-1}=g^{-1}\ast g = e$
    \end{itemize}
    
    An \textbf{Abelian Group} is one where $a * b = b * a$ (commutative)
\end{rcl}

\begin{dfn}[Definition of a field]{dfn:field}{}
    A {field} $F$ is a set with two functions
    \begin{itemize}
        \setlength\itemsep{0em}
        \item Addition: $+ : F \times F \to F,\,(\lambda, \mu) \mapsto \lambda + \mu$
        \item Multiplication: $\cdot : F \times F,\, (\lambda, \mu) \mapsto \lambda\mu$
    \end{itemize}
    which satisfy the following axioms:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $(F, +)$ is an abelian group $F^{+}$, with identity $0_{F}$
        \item $(F\backslash \{0_{F}\}, \cdot)$ is an abelian group $F^{\times}$, with identity $1_{F}$
        \item \textbf{Distributive law}: For all $a$, $b$, and $c$ in $F$, we have
            \[a(b + c) = ab + ac \in F\]
    \end{enumerate}
    and the following lemmas:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item The elements $0_{F}$ and $1_{F}$ of $F$ are distinct
        \item For all $a\in F$, $a \cdot 0_{F} = 0_{F} $ and $0_{F} \cdot a = 0_{F}$
        \item Multiplication in $F$ is associative, and $1_{F}$ is an identity element
    \end{enumerate}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}

    A \textbf{vector space $V$ over a field} $F$ is a pair consisting of an abelian group $V = (V,\, \dot{+})$ and a mapping
    \[F \times V \to V : (\lambda, \vec{v})\mapsto \lambda \vec{v}\]
    s.t. for all $\lambda, \mu \in F$ and $\vec{v}, \vec{w}\in V$ the following identities hold:

    \begin{itemize}
        \setlength\itemsep{0em}
        \item \textbf{Distributivity 1}: $\lambda(\vec{v} \dot{+} \vec{w}) = \lambda\vec{v} \dot{+} \lambda \vec{w}$
        \item \textbf{Distributivity 2}: $(\lambda + \mu)\vec{v} = \lambda \vec{v} \dot{+} \mu \vec{v}$
        \item \textbf{Associativity}: $\lambda (\mu \vec{v}) = (\lambda \mu) \vec{v}$
        \item \textbf{Identity}: $1\vec{v} = \vec{v}$
    \end{itemize}
    and so do the following lemmas:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item If $V$ is a vector space and $\vec{v}\in V$, then $0 \vec{v} = \vec{0}$
        \item If $V$ is a vector space and $\vec{v}\in V$, then $(-1)\vec{v} = -\vec{v}$
        \item If $V$ is a vector space over a field $F$, then $\lambda \vec{0} = \vec{0}$ for all $\lambda\in F$. Furthermore, if $\lambda \vec{v} = \vec{0}$ then either $\lambda = 0$ or $\vec{v} = \vec{0}$
    \end{enumerate}
\end{dfn}

\subsection{Working with Vector Spaces}

\begin{dfn}[Cartesian Product of $n$ sets]{dfn:cartesian-prod}{}
    \[X_{1} \times \cdots \times X_{n} := \{(x_{1}, \dots, x_{n}) : x_{i}\in X_{i} \text{ for } 1 \le i \le n\}\]

    The elements of a product are called \textbf{$n$-tuples}. An individual entry $x_{i} = (x_{1}, \dots ,x_{n})$ is called a \textbf{component}.

    There are special mappings called \textbf{projections} for a cartesian product:
    \begin{align*}
        \text{pr}_{i} : X_{1} \times \cdots \times X_{n} &\to X_{i}\\
        (x_{1},\dots,x_{n}) &\mapsto x_{i}
    \end{align*}

    The cartesian product of $n$ copies of a set $X$ is written in short as: $X^{n}$
\end{dfn}

\begin{dfn}[Vector Subspace]{dfn:vector-subspace}{}
    A subset $U$ of a vector space $V$ is called a \textbf{vector subspace} or \textbf{subspace} if $U$ contains the zero vector, and whenever $\vec{u},\vec{v}\in U$ and $\lambda\in F$ we have $\vec{u} + \vec{v}\in U$ and $\lambda \vec{u}\in U$
\end{dfn}

\begin{dfn}[Spans and Linear Independence]{dfn:spanning-subspace}{}
    Let $T \subset V$ for some vector space $V$ over a field $F$. Then amongus all subspaces of $V$ that include $T$ there is a smallest subspace
    \[\langle T \rangle = \langle T \rangle_{F} \subseteq V\]
    ``the set of all vectors $\alpha_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r}$ with $\alpha_{1},\dots,\alpha_{r}\in F$ and $\vec{v}_{1},\dots,\vec{v}_{r}\in T$, together with the zero vector in the case $T = \emptyset$''

    \noindent\rule{\textwidth}{0.2pt}

    \textbf{Terminology Dump}
    \begin{itemize}
        \setlength\itemsep{0em}
        \item An expression of the form $\alpha_{1}\vec{v}_{1}+\cdots+\alpha_{r}\vec{v}_{r}$ is called a \textbf{linear combination} of vectors $\vec{v}_{1},\dots,\vec{v}_{r}$
        \item The smallest vector subspace $\langle T \rangle \subseteq V$ containing $T$ is called the \textbf{vector subspace generated by $T$} or the vector subspace \textbf{spanned by $T$} or even the \textbf{span of $T$}
        \item If we allow the zero vector to be the "empty linear combination of $r = 0$ vectors", then the span of $T$ is exactly the set of all linear combinations of vectors from $T$ 
        \item A subset of a vector space that spans the entire space is called a \textbf{generating} or \textbf{spanning set}. A vector space that has a finite generating set is said to be \textbf{finitely generated}
    \end{itemize}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}

    \textbf{Linear Independence}
    \vspace{5pt}

    A subset $L$ of a vector space $V$ is called \textbf{linearly independent} if for all pairwise different vectors $\vec{v}_{1},\dots,\vec{v}_{r}\in L$ and arbitrary scalars $\alpha,\dots,\alpha_{r}\in F$,
    \[a_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r} = \vec{0} \implies a_{1} = \cdots = \alpha_{r} = 0\]
    A subset $L$ of a vector space $V$ is called \textbf{linearly dependent} if it is not linearly independent (duh..). This means there exists pairwise different vectors $\vec{v}j_{1},\dots,\vec{v}_{r}\in L$ and scalars $\alpha_{1},\dots,\alpha_{r}\in F$, not all zero, such that $\alpha_{1}\vec{v}_{1} + \cdots \alpha_{r}\vec{v}_{r} = \vec{0}$
\end{dfn}

\subsection{Linear Independence and Bases}

\begin{dfn}[Basis of a Vector Space]{dfn:basis}{}
    A \textbf{basis of a vector space} $V$ is a linearly independent generating set in $V$
\end{dfn}

\begin{xmp}[Standard Basis]{xmp:standard-basis}{}
    Let $F$ be a field and $n\in \mathbb{N}$. We consider the following vectors in $F^{n}$
    \[\vec{e}_{i} = (0,\dots,0,1,0,\dots,0)\]
    with one $1$ in the $i$-th place and zero everywhere else. Then $\vec{e}_{1} ,\dots, \vec{e}_{n}$ form an ordered basis of $F^{n}$, the so-called \textbf{standard basis of $F^{n}$}
\end{xmp}

\begin{thm}[Linear combinations of basis elements]{thm:linear-combinations-of-basis-elems}{}
    Let $F$ be a field, $V$ a vector space over $F$ and $\vec{v}_{1},\dots,\vec{v}_{r}\in V$ vectors. The family $(\vec{v}_{i})_{1\le i\le r}$ is a basis of $V$ if and only if the following "evaluation" mapping
    \begin{align*}
        \psi : F^{r} &\to V\\
        (\alpha_{1},\dots,a_{r}) &\mapsto a_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r}
    \end{align*}
    is a bijection

    If we label our ordered family by $\mathcal{A} = (\vec{v}_{1},\dots,\vec{v}_{r})$, then we done the above mapping by
    \[\psi = \psi_{\mathcal{A}} : F^{r}\to V\]
\end{thm}

\begin{thm}[Characterisations of Bases]{thm:basis-characterisations}{}
    The following are equivalent for a subset $E$ of a vector space $V$:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $E$ is a basis, i.e. a linearly independent generating set
        \item $E$ is minimal among all generating sets, meaning that $E \backslash \{\vec{v}\}$ does not generate $V$, for any $\vec{v}\in E$
        \item $E$ is maximal among all linearly independent subsets, meaning that $E \cup \{\vec{v}\}$ is linearly dependent for any $\vec{v}\in V$
    \end{enumerate}

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Corrollary}: Let $V$ be a finitely generated vector space over a field $F$. Then $V$ has a finite basis

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Basis Characterisation Variant}
    \begin{enumerate}
        \item If $L \subset V$ is a linearly independent subset and $E$ is minimal amongst all generating sets of $V$ with the property that $L \subseteq E$, then $E$ is a basis.
        \item If $E \subseteq V$ is a generating set and if $L$ is maximal amongst all linearly independent sets of $V$ with the property $L \subseteq$ $E$, then $L$ is a basis.
    \end{enumerate}
\end{thm}

\newpage

\begin{dfn}[Free Vector Space]{dfn:free-vector-space}{}
    Let $X$ be a set and $F$ a field. The set $\Maps(X, F)$ of all mappings $f: X \to F $ becomes an $F$-vector space with the operations of pointwise addition and multiplication by a scalar. The subset of all mappings which send almost all elements of $X$ to zero is a vector subspace
    \[F\langle X \rangle \subseteq \Maps(X, F)\]
\end{dfn}


\section{Rings}
I can't be bothered doing changes of basis and stuff, time for something more interesting :D 

\subsection{Ring basics}
\begin{dfn}[Definition of a Ring]{dfn:ring}{}
    A \textbf{ring} is a set with two operations $(\mathbb{R}, +, \cdot)$ that satisfy:
    \begin{enumerate}
        \item $(R, +)$ is an abelian group
        \item $(R, \cdot)$ is a \textbf{monoid} - this means that the second operation $\cdot : R \times R \to R$ is associative and that there is an \textbf{identity element} $1 = 1_{R}\in R$, often just called the identity, with the property that $1 \cdot a = a \cdot 1 = a$ for all $a\in R$.
        \item The distributive laws hold, meaning that for all $a,b,c\in R$,
            \begin{align*}
                a \cdot (b + c) &= (a \cdot b) + (a \cdot c)) \\
                (a + b) \cdot c &= (a \cdot c) + (b \cdot c)
            \end{align*}
    \end{enumerate}
    The two operations are called \textbf{addition} and \textbf{multiplication} in our ring. A ring in which multiplication, that is $a \cdot b = b \cdot a$ for all $a,b\in R$, is a \textbf{commutative ring}
\end{dfn}

\textbf{Note}: We'll call the element $1\in R$ as the identity element of the monoid $(R, \cdot)$, and we call the additive identity of $(R, +)$ zero, written as $0_{R}$ or $0$

\textbf{Example}: We can define the \textbf{null ring} or \textbf{zero ring} as a ring where $R$ is a single ement set, e.g. $\{0\}$, with the operations $0 + 0 = 0$ and $0 \times 0 = 0$. We will call any ring that isn't the zero ring a \textbf{non-zero ring}

\begin{xmp}[Modulo Rings]{xmp:modulo-rings}{}
    Let $m\in \mathbb{Z}$ be an integer. Then the set of \textbf{integers modulo} $m$, written
    \[\mathbb{Z} / m\mathbb{Z}\]
    is a ring. The elements of $\mathbb{Z} / m\mathbb{Z}$ consist of \textbf{congruence classes} of integers modulo $m$ - that is the elements are the subsets $T$ of $\mathbb{Z}$ of the form $T = a + m\mathbb{Z}$ with $a\in \mathbb{Z}$. Think of these as the set of integers that have the same remainder when you divide them by $m$. I denote the above congruence class by $\overline{a}$. Obviously $\overline{a} = \overline{b}$ is the same as $a-b\in m\mathbb{Z}$, and often I'll write
    \[a \equiv b \mod m\]
\end{xmp}

\subsection{Linking Rings to Fields and Further Properties}

\begin{dfn}[Ring definition of a field]{dfn:field-ring}{}
    A \textbf{field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1}\in F$, that is an element $a^{-1}$ with the property that $a \cdot a^{-1} = a^{-1} \cdot a = 1$
\end{dfn}

\begin{thm}[Prime property of fields]{thm:prime-ring-fields}{}
    Let $m$ be a positive integer. The commutative ring $\mathbb{Z} / m\mathbb{Z}$ is a field if and only if $m$ is prime.
\end{thm}

\begin{thm}[Lemmas for multiplying]{dfn:ring-lemmas-1}{}
    Let $R$ be a ring and let $a,b\in R$. Then
    \begin{enumerate}
        \item $0a = 0 = a 0$
        \item $(-a)b = -(ab) = a(-b)$
        \item $(-a)(-b) = ab)$
    \end{enumerate}
\end{thm}


\begin{dfn}[Multiples of an abelian group]{dfn:abelian-group-multis}{}
    Let $m\in \mathbb{Z}$. The \textbf{$m$-th multiple $ma$ of an element $a$}in an abelian group $R$ is:
    \[ma = \underbrace{a + a + \cdots + a}_{\text{$m$ terms}} \quad \text{if} m > 0\]
    $0a = 0$ and negative multiples are defined by $(-m)a = -(ma)$
\end{dfn}

\begin{thm}[Lemmas for multiples]{dfn:ring-lemmas-2}{}
    Let $R$ be a ring, let $a,b\in R$ and let $m,n\in \mathbb{Z}$. Then:
    \begin{enumerate}
        \item $m(a + b) = ma + mb$
        \item $(m + n)a = ma + na$
        \item $m(na) = (mn)a$
        \item $m(ab) = (ma)b = a(mb)$
        \item $(ma)(nb) = (mn)(ab)$
    \end{enumerate}
\end{thm}

\begin{dfn}[Unit of a ring]{dfn:ring-unit}{}
    Let $R$ be a ring. An element $a\in R$ is called a \textbf{unit} if it is \textit{invertible} in $R$ or in other words \textit{has a multiplicative inverse in $R$}, meaning that there exists $a^{-1}\in R$ such that
    \[aa^{-1} = 1 = a^{-1} a\]
\end{dfn}

\begin{thm}[]{thm:ring-units-form-a-group}{}
    The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication
\end{thm}

\begin{dfn}[zero-divisors of a ring]{dfn:zero-divisor}{}
    In a ring $R$, a non-zero element $a$ is called a \textbf{zero-divisor} or \textbf{divisor of zero} if there exists a non-zero element $b$ such that either $ab = 0$ or $ba = 0$.
\end{dfn}

\begin{thm}[Cancellation Law]{thm:int-domains-cancellation-law}{}
    Let $R$ be an \textit{integral domain} and let $a,b,c\in R$. If $ab = ac$ and $a\ne 0$ then $b = c$
\end{thm}

\begin{thm}[Prime Property for Integral Domains]{thm:prime-int-domains}{}
    Let $m$ be a natural number. Then $\mathbb{Z} / m\mathbb{Z}$ is an integral domain if and only if $m$ is prime.
\end{thm}

\begin{thm}[]{thm:finite-int-domains-are-fields}{}
    Every \textbf{finite} integral domain is a \hyperref[dfn:field]{field}.
\end{thm}

\subsection{Polynomials}

\begin{dfn}[Polynomial]{dfn:polynomial}{}
    Let $R$ be a ring. A \textbf{polynomial over $R$} is an expression of the form
    \[P = a_{0} + a_{1}X + a_{2}X^{2} + \cdots + a_{m}X^{m}\]
    for some non-negative integer $m$ and elements $a_{i}\in R$ for $0 \le i \le m$. The set of all polynomials over $R$ is denoted by $R[X]$. In the case where $a_{m}$ is non-zero, the polynomial $P$ has \textbf{degree} $m$, (written $\deg(P)$), and $a_{m}$ is its \textbf{leading coefficient}. When the leading coefficient is $1$ the polynomial is a \textbf{monic polynomial}. A polynomial of degree one is called \textbf{linear}, a polynomial od degree two is called \textbf{quadractic}, and a polynomial of degree three is called \textbf{cubic}.
\end{dfn}

\begin{dfn}[Ring of Polynomials]{dfn:polynomial-rings}{}
    The set $R[X]$ becomes a ring called the \textbf{ring of polynomials with coefficients in $R$, or over $R$}. The zero and the identity of $R[X]$ are the zero and identity of $R$, respectively.
\end{dfn}


\begin{thm}[Zero-Divisors of a Polynomial Ring]{thm:zero-divisors-of-poly-ring}{}
    If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors and $\deg(PQ) = \deg(P) + \deg(Q)$ for non-zero $P,Q\in R[X]$.
    
    \noindent\rule{\textwidth}{0.2pt}
    If $R$ is an integral domain, then so is $R[X]$
\end{thm}

\newpage
\begin{thm}[Division and Remainder]{thm:division-and-remainder}{}
    Let $R$ be an integral domain and let $P, Q\in R[X]$ with $Q$ monic. Then there exists unique $A,B\in R[X]$ such that $P = AQ + B$ and $\deg(B) < \deg(Q)$ or $B = 0$
\end{thm}

\begin{dfn}[Formal definition of a function]{dfn:function-poly-ring}{}
    Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$ can be \textbf{evaluated} at the element $\lambda\in R$ to produce $P(\lambda)$ by replacing the powers of $X$ in the polynomial $P$ by the corresponding powers of $\lambda$. In this way we have a mapping
    \[R[X] \to \Maps(R, R)\]
    This is the precise mathematical description of thinking of a polynomial as a function. An element $\lambda\in R$ is a \textbf{root} of $P$ is $P(\lambda) = 0$
\end{dfn}

\begin{thm}[Roots of a Polynomial]{thm:polynomial-roots}{}
    Let $R$ be a commutative ring, let $\lambda\in R$ and $P(X) \in R[X]$. Then $\lambda$ is a root of $P(X)$ if and only if $(X - \lambda)$ divides $P(X)$
\end{thm}

\begin{thm}[Degrees of Polynomial Roots]{thm:polynomial-root-degs}{}
    Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial $P\in R[X] \backslash \{0\}$ has at most $\deg(P)$ roots in $R$
\end{thm}

\begin{dfn}[Algebraically closed fields]{dfn:algebraically=closed}{}
    A field $F$ is \textbf{algebraically closed} if each non-constant polynomial $P\in F[X]\backslash F$ with coefficients in our field has a root in our field $F$
\end{dfn}

\begin{thm}[Fundamental Theorem of Algebra]{thm:fundamental-theorem-of-algebra}{}
    The field of complex numbers $\mathbb{C}$ is algebraically closed.
\end{thm}

\begin{thm}[Linear Factors of Closed Fields]{thm:alg-closed-fields-linear-factors}{}
    If $F$ is an algebraically closed field, then every non-zero polynomial $P\in F[X]\backslash \{0\}$ \textbf{decomposes into linear factors}
    \[P = c(X - \lambda_{1}) \cdots (X - \lambda_{n})\]
    with $n\ge 0,\, c\in F^{\times}$ and $\lambda_{1},\dots,\lambda_{n}\in F$. This decomposition is unique up to reordering the factors
\end{thm}

% TODO: bunch of stuff :)))))))


\section{Determinants and Eigenvalues Redux}

\subsection{Symmetric Groups}

\begin{dfn}[Symmetric Groups]{dfn:symmetric-groups}{}
    The group of all permutations of the set $\{1,2,\dots,n\}$, also known as bijections from $\{1,2,\dots,n\}$ to itself is denoted by $\mathfrak{S}_{n}$ (but i will just write $S_{n}$ because icba) and called the \textbf{$n$-th symmetric group}. It is a group under composition and has $n!$ elements.

    \noindent\rule{\textwidth}{0.2pt}
    A \textbf{tranposition} is a permutation that swaps two elements of the set and leaves all the others unchanged.
\end{dfn}

\begin{dfn}[Inversions of a permutation]{dfn:inversion}{}
    An \textbf{inversion} of a permutation $\sigma\in S_{n}$ is a pair $(i, j)$ such that $1 \le i < j \le n$ and $\sigma(i) > \sigma(j)$. The number of inversions of the permutation $\sigma$ is called the \textbf{length of $\sigma$} and written $\ell(\sigma)$. In formulas:
    \[\ell(\sigma) = \lvert \{(i,j) : i < j \text{ but } \sigma(i) > \sigma(j)\} \rvert\]
    The \textbf{sign of $\sigma$} is defined to be the parity of the number of inversions of $\sigma$. In formulas:
    \[\sgn(\sigma) = (-1)^{\ell(\sigma)}\]
\end{dfn}

\begin{thm}[Multiplicativity of the sign]{thm:permutation-multiplicativity}{}
    For each $n\in \mathbb{N}$ the sign of a permutation produces a group homomorphism $\sgn : S_{n} \to \{+1, -1\}$ from the symmetric group to the two-element group of signs. In formulas:
    \[\sgn(\sigma\tau) = \sgn(\sigma)\sgn(\tau) \quad \forall \sigma, \tau\in S_{n}\]
\end{thm}

\begin{dfn}[Alternating Group of a Permutation]{dfn:alternating-group}{}
    For $n\in \mathbb{N}$, the set of even permutations in $S_{n}$ forms a subgroup of $S_{n}$ because it is the kernel of the group homomorphism $\sgn : S_{n}\to \{+1, -1\}$. This group is the \textbf{alternating group} and is denoted $A_{n}$
\end{dfn}

\subsection{Determinants}

\begin{dfn}[Determinants]{dfn:determinants}{}
    Let $R$ be a commutative ring and $n\in \mathbb{N}$. The \textbf{determinant} is a mapping $\det : \Mat(n;R) \to R$ from square matrices with coefficients in $R$ to the ring $R$ that is given by the following formula

    \[A = \begin{pmatrix}
        a_{11} & \cdots & a_{1n}\\
        \vdots & \ddots & \vdots\\
        a_{n1} & \cdots & a_{nn}
    \end{pmatrix} \mapsto \det(A) = \sum_{\sigma\in S_{n}} \sgn(\sigma) a_{1\sigma(1)\cdots}  a_{n\sigma(n)}\]
\end{dfn}

\subsection{Characterising the Determinant}

\begin{dfn}[Bilinear Forms]{dfn:bilinear-forms}{}
    Let $U,V,W$ be $F$-vector spaces. A \textbf{bilinear form on $U \times V$ with values in $W$} is a mapping $H: U \times V \to W $ which is a linear mapping in both of its entries. This means that it must satisfy the following properties for all $u_{1}, u_{2}\in U$ and $v_{1}, v_{2}\in V$ and all $\lambda\in F$:
    \begin{align*}
        H(u_{1} + u_{2}, v_{2}) &= H(u_{1}, v_{1}) + H(u_{2}, v_{1})\\
        H(\lambda u_{1}, v_{1}) &= \lambda H(u_{1}, v_{1}) \\
        H(u_{1}, v_{2} + u_{2}) &= H(u_{1}, v_{1}) + H(u_{2}, v_{1})\\
        H(u_{1},\lambda v_{1}) &= \lambda H(u_{1}, v_{1}) \\
    \end{align*}
\end{dfn}

\begin{dfn}[Multilinear Forms]{dfn:multilinear}{}
    Let $V_{1},\dots,V_{n}, W$ be $F$-vector spaces. A mapping $H : V_{1} \times V_{2} \times \cdots \times V_{n} \to W$ is a \textbf{multilinear form} or just \textbf{multilinear} if for each $j$, the mapping $V_{j}\to W$ defined by $v_{j}\mapsto H(v_{1},\dots,v_{j},\dots,v_{n})$, with the $v_{i}\in V_{i}$ arbitrary fixed vectors of $V_{i}$ for $i\ne j$ is linear. 
\end{dfn}

\begin{dfn}[Alternating Multilinear Forms]{dfn:alternating-multilinear}{}
    Let $V$ and $W$ be $F$-vector spaces. A multilinear form $ H : V \times \cdots \times V \to W$ is \textbf{alternating} if it vanishes on every $n$-tuple of elements of $V$ that has at least two entries equal, in other words if:
    \[(\exists i\ne j \text{ with } v_{i} = v_{j})\to H(v_{1},\dots,v_{i},\dots,v_{j},\dots,v_{n}) = 0\]
\end{dfn}

\begin{thm}[Characterisation of the Determinant]{thm:determinant-characterisation}{}
    Let $F$ be a field. The mapping
    \[\det : \Mat(n;F) \to F\]
    is the unique alternating multilinear form on $n$-tuples of column vectors with values in $F$ that takes the value $1_{F}$ on the identity matrix
\end{thm}

\subsection{Rules for Calculating with Determinants}

\begin{thm}[Multiplicativity of the Determinant]{thm:determinant-multiplicativity}{}
    Let $R$ be a commutative ring and let $A,B\in \Mat(n;R)$. Then
    \[\det(AB) = \det(A)\det(B)\]
\end{thm}

\begin{thm}[]{thm:invertibility-criteron}{}
    The determinant of a square matrix with entries in a field $F$ is non-zero if and only if the matrix is invertible
\end{thm}

\newpage
\subsubsection{Consequences of determinant rules}
\begin{itemize}
    \item If $A$ is invertible then $\det(A^{-1}) = \det(A)^{-1}$
    \item If $B$ is a square matrix then $\det(A^{-1}BA) = \det(B)$
\end{itemize}



\begin{thm}[Determinants of a Transpose Matrix]{thm:determinant-of-transpose}{}
    The determinant of a square matrix and of the transpose of the square matrix are equal, that is for all $A\in \Mat(n;R)$ with $R$ a commutative ring,
    \[\det(A^{T}) = \det(A)\]
\end{thm}

% \subsubsection{Simplifying Determinant Calculations}
% Via \ref{thm:determinant-characterisation}, the theorem demonstrates that Gaussian elimination works with finding determinants - \textit{Row Addition} doesn't change the determinant, while \textit{Row Swap} changes the sign only. When Gaussian elimination is completed, the final matrix has staircase form so is upper triangular from which the determinant can easily be calculated by multiplying the entries of the diagonal together.

% \noindent\rule{\textwidth}{0.2pt}
% Although the determinant $\det(A) = \sum_{\sigma\in S_{n}} \sgn(\sigma) \prod_{i = 1}^{n} a_{i\sigma(i)}$ might strike you as difficult to calculate, the superficially simpler expression $\text{per}(A) = \sum_{\sigma\in S_{n}}\prod_{i=1}^{n} a_{i\sigma(i)}$ known as the \textbf{permanent} is much harder to calculate, and it's known to be NP-hard to compute.

\begin{dfn}[Cofactors of a Matrix]{dfn:cofactors-matrix}{}
    Let $A \in \Mat(n;R)$ for some commutative ring $R$ and natural number $n$. Let $i$ and $j$ be integers between $1$ and $n$. Then the $(i, )$ \textbf{cofactor of $A$} is $C_{ij} = (-1)^{i + j} \det(A\langle i,j \rangle)$ where $A\langle i, j \rangle$ is the matrix obtained from $A$ by deleting the $i$-th row and $j$-th column.
    \[C_{23} = (-1)^{2 + 3} \det \begin{pmatrix}
        a_{11} & a_{12}& \textcolor{red}{a_{13}}\\
        \textcolor{red}{a_{21}}& \textcolor{red}{a_{22}}& \textcolor{red}{a_{23}}\\
        a_{31}& a_{32}& \textcolor{red}{a_{33}}
    \end{pmatrix} = -a_{11}a_{32} + a_{31}a_{12}\]
\end{dfn}

\begin{thm}[Laplace's Expansion]{thm:laplace-determinant}{}
    Let $A = (a_{ij})$ be an $(n \times n)$-matrix with entries from a commutative ring $R$. For a fixed $i$, the \textbf{$i$-th row expansion of the determinant} is
    \[\det(A) = \sum_{j = 1}^{n}a_{ij}C_{ij}\]
    and for a fixed $j$, the \textbf{$j$-th column expansion of the determinant} is
    \[\det(A) = \sum_{i = 1}^{n} a_{ij} C_{ij}\]
\end{thm}

\begin{dfn}[Adjugate Matrix]{dfn:adjugate-matrix}{}
    Let $A$ be a $(n \times n)$-matrix with entries in a commutative ring $R$. The \textbf{adjugate matrix} $\text{adj}(A)$ is the $(n \times n)$-matrix whose entries are $adj(A)_{ij} = C_{ji}$ where $C_{ji}$ is the $(j, i)$-cofactor
\end{dfn}

\begin{thm}[Cramer's Rule]{thm:cramers-rule}{}
    Let $A$ be a $(n \times n)$-matrix with entries in a commutative ring $R$. Then
    \[A \cdot \text{adj}(A) = (\det A)I_{n}\]
\end{thm}

\subsubsection{Alternative Definition of Cramer's}
In many sources, such as Wikipedia, Cramer's Rule means the formula
\[x_{i} = \frac{\det(a_{*1}\mid \dots \mid b_{*} \mid \dots \mid a_{*n})}{\det(a_{*1}\mid \dots \mid a_{*i} \mid \dots \mid a_{*n})}\]
for solving a field $F$ the system $A\vec{x} = \vec{b}$ of $n$ linear equations in $n$ unknowns, provided that a unique solution exists. A unique solution exists if and only if $A$ is invertible. So, instead of applying the Gaussian algorithm, you can calculate lots of determinants, replacing the $i$-th column of $A$ by the given solution vector $\vec{b}$. It turns out that if you implement this rule on a computer, it has the same efficiency as the Gaussian algorithm. The relationship between this version of Cramer's rule and the above theorem is got by successively taking the vector $\vec{b}$ in the system of linear equations to be the standard basis elements $\vec{e_{i}}$ with $1 \le i \le n$.

\begin{thm}[Invertibility of Matrices]{thm:invertibility-of-matrices}{}
    A square matrix with entries in a commutative ring $R$ is invertible if and only if its determinant is a unit in $R$. That is, $A\in \Mat(n;R)$ is invertible if and only if $\det(A)\in R^{\times}$
\end{thm}

So for instance, an integral matrix $A\in \Mat(n;\mathbb{Z})$ is invertible if and only if $\det(A)$ is $1$ or $-1$, since $\mathbb{Z}^{\times} = \{\pm 1\}$. On the other hand, a matrix $A\in \Mat(n;F)$ with entries in a field $F$ is invertible if and only if $\det(A)\ne 0$ since $F^{\times}$ consists of the non-zero elements of $F$.

\begin{thm}[Jacobi's Formula]{thm:jacobis}{}
    Let $A = (a_{ij})$ where the coefficients $a_{ij} = a_{ij}(t)$ are functions of $t$. Then
    \[\frac{d}{dt} \det A = \text{Tr}\text{Adj} A \frac{dA}{dt}\]
\end{thm}

\subsection{Eigenvalues and Eigenvectors}

\begin{dfn}[Eigenvalues and Eigenvectors]{dfn:eigenvalue-eigenvector}{}
    Let $f: V \to V $ be an endomorphism of an $F$-vector space $V$. A scalar $\lambda\in F$ is an \textbf{eigenvalue of $f$} if and only if there exists a non-zero vector $\vec{v}\in V$ such that $f(\vec{v}) = \lambda \vec{v}$. Each such vector is called an \textbf{eigenvector of $f$ with eigenvalue $\lambda$}. For any $\lambda\in F$, the \textbf{eigenspace of $f$ with eigenvalue $\lambda$} is
    \[E(\lambda, f) = \{\vec{v}\in V : f(\vec{v}) = \lambda \vec{v}\}\]
\end{dfn}

\begin{thm}[Existence of Eigenvalues]{thm:existence-of-eigenvalues}{}
    Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field has an eigenvalue
\end{thm}

\begin{dfn}[Characteristic Polynomial]{dfn:characteristic-polynomial}{}
    Let $R$ be a commutative ring and let $A\in \Mat(n;R)$ be a square matrix with entries in $R$. The polynomial $\det(x I_{n} - A)\in R[x]$ is called the \textbf{characteristic polynomial of the matrix $A$}. It is denoted by
    \[\chi_{A}(x) := \det(x I_{n} - A)\]
    (where $\chi$ stands for $\chi$aracteristic, lol)
\end{dfn}

\begin{thm}[EVs and Characteristic Polynomials]{thm:eigenvalues-char-polynomial}{}
    Let $F$ be a field and $A\in \Mat(n;F)$ a square matrix with entries in $F$. The eigenvalues of the linear mapping $A : F^{n}\to F^{n}$ are exactly the roots of the characteristic polynomial $\chi_{A}$
\end{thm}

\subsubsection{Eigenvalue remarks}
\begin{enumerate}
    \item Square matrices $A,\,B\in \Mat(n;R)$ of the same size are \textit{conjugate} if
        \[B = P^{-1}AP\in \Mat(n;R)\]
    for an invertible $P\in \text{GL}(n;R)$. Conjugacy is an equivalence relation on $\Mat(n;R)$. (The definition makes sense for any commutative ring $R$, although we will mainly be concerned with the case of a field)
    \item The motivation for conjugacy comes from the various matrix representations for an endomorphism $f : V \to V$ of an $n$-dimensional vector space $V$ over a field $F$. Let
        \[A = (a_{ij}) =\,_{\mathcal{A}}[f]_{\mathcal{A}},\,B = (b_{ij}) =\,_{\mathcal{B}}[f]_{\mathcal{B}}\in \Mat(n;f)]\]
        be the matrices of $f$ with respect to bases $\mathcal{A} = (\vec{v_{1}}, \vec{v_{2}},\dots,\vec{v_{n}}),\,\mathcal{B} = (\vec{w_{1}}, \vec{w_{2}},\dots,\vec{w_{n}})$ for $V$
        \[f(\vec{v_{j}}) = \sum_{i = 1}^{n}a_{ij}\vec{v_{i}},\,f(\vec{w_{j}}) = \sum_{i = 1}^{n}b_{ij}\vec{w_{i}}\in V\]

        The change of basis matrix $P = (p_{ij}) =\,_{\mathcal{A}} [\text{id}_{V}]_{\mathcal{B}}\in \Mat(n;F)$ is invertible, with
        \[\vec{w_{j}} = \sum_{i = 1}^{n}p_{ij}\vec{v_{i}}\in V\]
        We have the identity
        \[B = P^{-1}AP\in \Mat(n;F)\]
        so $A,B$ are conjugate
    \item \textbf{Key observation}: the characteristic polynomials of conjugate $A,B\in \Mat(n,R)$ are the same
        \begin{align*}
            \chi_{B}(x) &= \det(x I_{n} - B) = \det(x I_{n} = P^{-1}AP) \\
            &= \det(P^{-1}(x I_{n} - A)P) = \det(P)^{-1}\det(x I_{n} - A)\det(P) \\
            &= \det(xI_{n} - A) = \chi_{A}(x)\in R[x]
        \end{align*}
    \item In view of $2$ and $3$ we can define the characteristic polynomial of an endomorphism $f : V\to V$ of an $n$-dimensional vector space over a field $F$ to be
        \[\chi_{f}(x) = \chi_{A}(x)\in F[x]\]
        with $A =\mathcal{A}\,[f]_{\mathcal{A}}\in \Mat(n;R)$ the matrix of $f$ with respect to \textit{any} basis $\mathcal{A}$ for $V$. Thanks to \ref{thm:eigenvalues-char-polynomial}, the eigenvalues of $f$ are exactly the roots of $\chi_{f}$, the characteristic polynomial of $f$
\end{enumerate}

\textbf{Remark}: Let $f : V\to V$ be an endomorphism of an $n$-dimensional vector space $V$ over a field $F$. Suppose given an $m$-dimensional subspace $W\subseteq V$ such that $f(W)\subseteq W$, so that there are defined endomorphisms of the subspace and the quotient space
\begin{align*}
    g &: W \to W;\, \vec{w}\mapsto f(\vec{w})\\
    h &: V / W \to V / W;\, W + \vec{v} \mapsto W + f(\vec{v})
\end{align*}
Any ordered basis $\mathcal{A} = (\vec{w_{1}},\vec{w_{2}},\dots,\vec{w_{m}})$ for $W$ can be extended to an ordered basis for $V$
\[\mathcal{B} = (\vec{w_{1}}, \vec{w_{2}},\dots,\vec{w_{m}},\vec{v}_{m+1},\vec{v}_{m+2}\cdots,\vec{v_{n}})\]

The images of the $\vec{v}_{j}$'s under the canonical projection $\text{can} : V\to V / W$ are then an ordered basis for $V / W$
\[\mathcal{C} = (\text{can}(v_{m + 1}), \text{can}(v_{m + 2}),\dots,\text{can}(\vec{v}_{n}))\]
Let $a_{ij},b_{jk},c_{ik}\in F$ be the coefficients in the linear combinations
\[f(\vec{w}_{j}) = \sum_{i = 1}^{m}a_{ij}\vec{w}_{i}\in W,\,f(\vec{v}_{k}) = \sum_{j = m+1}^{n} b_{jk}\vec{v}_{j} + \sum_{i = 1}^{n}c_{ik}\vec{w}_{i}\in V\]

[WIP SO MUCH WRITING OMG]

\subsection{Triangularisable, Diagonalisable, and Cayley-Hamilton}

\begin{dfn}[Triangularisability]{dfn:triangularisability}{}
    Let $f : V \to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. $f$ is \textbf{triangularisable} if the vector space $V$ has an ordered basis $\mathcal{B} = (\vec{v}_{1}, \vec{v}_{2},\dots,\vec{v}_{n})$ such that
        \begin{align*}
            f(\vec{v}_{1}) &= a_{11}\vec{v_{1}}, \\
            f(\vec{v_{2}}) &= a_{12}\vec{v}_{1} + a_{22}\vec{v}_{2}, \\
            &\vdots \\
            f(\vec{v}_{n}) &= a_{1n}\vec{v}_{1} + a_{2n}\vec{v}_{2} + \cdots + a_{nn}\vec{v}_{n}\in V
        \end{align*}
        (so that the first basis vector $\vec{v}_{1}$ is an eigenvector, with eigenvalue $a_{11}$) or equivalently such that the $n \times n$ matrix $_{\mathcal{B}}[f]_{\mathcal{B}} = (a_{ij})$ representing $f$ with respect to $\mathcal{B}$ is upper triangular (or any other triangular)
        \[A = \begin{pmatrix}
            a_{11}& a_{12}& a_{13}& \cdots & a_{1n} \\
            0& a_{22}& a_{23}& \cdots& a_{2n} \\
            0 & 0& a_{33}& \cdots& a_{3n} \\
            \vdots& \vdots& \vdots& \ddots& \vdots \\
            0 & 0& 0& \cdots& a_{nn}
        \end{pmatrix}\]
\end{dfn}


\begin{thm}[]{thm:triangularisability-chi-poly}{}
    Let $f : V \to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. Then $f$ is triangularisable iff the characteristic polynomial $\chi_{f}$ decomposes into linear factors in $F[x]$
\end{thm}

\begin{thm}[Triangularisability and Conjugacy]{thm:triangularisability-and-conjugacy}{}
    An endomorphism $A : F^{n} \to F^{n}$ is triangularisable if and only if $A = (a_{ij})$ is conjugate to an upper triangular matrix $B = (b_{ij})(b_{ij} = 0 \text{ for $i > j$})$, with $P^{-1} AP=B$ for an invertible matrix $P$
\end{thm}

\begin{dfn}[Diagonalisability]{dfn:diagonal}{}
    An endomorphism $f : V \to V$ of an $F$-vector space $V$ is \textbf{diagonalisable} if and only if there exists a basis of $V$ consisting of eigenvectors of $f$. If $V$ is finite dimensional then this is the same as saying that there exists an ordered basis $\mathcal{B} = \{\vec{v}_{1},\dots,\vec{v}_{n}\}$ such that corresponding matrix representing $f$ is diagonal, that is $_{\mathcal{B}}[f]_{\mathcal{B}} = \text{diag}(\lambda_{1},\dots,\lambda_{n})$. In this case, of course, $f(\vec{v}_{i}) = \lambda_{i}\vec{v}_{i}$.
    
    A square matrix $A\in \Mat(n;F)$ is \textbf{diagonalisable} if and only if the corresponding linear mapping $F^{n}\to F^{n}$ given by left multiplication by $A$ is diagonalisable. Thanks to [something] this just means that $A$ is conjugate to a diagonal matrix, there exists an invertible matrix $P \in \text{GL}j(n;F)$ such that $P^{-1}AP = \text{diag}(\lambda_{1},\dots,\lambda_{n})$. In this case the columns $P$ are the vectors of a basis of $F^{n}$ consisting of eigenvectors of $A$ with eigenvalues $\lambda_{1},\dots,\lambda_{n}$
\end{dfn}

\begin{thm}[Linear Independence of Eigenvectors]{thm:ev-linear-independence}{}
    Let $f : V\to V$ be an endomorphism of a vector space $V$ and let $\vec{v}_{1},\dots,\vec{v}_{n}$ be eigenvectors of $f$ with pairwise different eigenvalues $\lambda_{1},\dots,\lambda_{n}$. Then the vectors $\vec{v}_{1},\dots,\vec{v}_{n}$ are linearly independent
\end{thm}

\begin{thm}[Cayley-Hamilton Theorem]{thm:cayley-hamilton}{}
    Let $A\in \Mat(n;R)$ be a square matrix with entries in a commutative ring $R$. Then evaluating its characteristic polynomial $\chi_{A}(x)\in R[x]$ at the matrix $A$ gives zero.
\end{thm}


\lipsum[1-12]

\end{multicols}

\end{document}
